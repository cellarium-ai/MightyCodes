{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebook Design via Variational Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from ruamel_yaml import YAML\n",
    "from ruamel_yaml.comments import CommentedMap\n",
    "import logging\n",
    "import pprint\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from boltons.cacheutils import cachedproperty, cachedmethod\n",
    "from typing import Dict, Any, Callable, Optional, Dict, List, Set, Tuple, Union\n",
    "\n",
    "from mighty_codes import consts\n",
    "from mighty_codes import metric_utils\n",
    "from mighty_codes import experiments\n",
    "\n",
    "from mighty_codes.torch_utils import \\\n",
    "    to_np, \\\n",
    "    to_torch, \\\n",
    "    to_one_hot_encoded\n",
    "\n",
    "from mighty_codes.experiments import \\\n",
    "    ChannelModelSpecification\n",
    "\n",
    "from mighty_codes.rl.mrf_one_body import \\\n",
    "    OneBodyMRFPotential, \\\n",
    "    NeuralPIOneBodyMRFPotential\n",
    "\n",
    "from mighty_codes.rl.mrf_two_body import \\\n",
    "    TwoBodyMRFPotential, \\\n",
    "    NeuralGeneralPITwoBodyMRFPotential, \\\n",
    "    TwoBodyPotentialPrefactor, \\\n",
    "    BiLinearTwoBodyPotentialPrefactor, \\\n",
    "    BiLinearTwoBodyPotentialPrefactorSimple, \\\n",
    "    NeuralTwoBodyPotentialPrefactor, \\\n",
    "    ExpBiLinearPITwoBodyMRFPotential\n",
    "\n",
    "from mighty_codes.rl.mrf_code_gen import \\\n",
    "    MRFCodebookGenerator, \\\n",
    "    code_space_filter_func_rel_symbol_weight, \\\n",
    "    code_space_filter_func_abs_hamming_weight\n",
    "\n",
    "\n",
    "yaml = YAML()\n",
    "yaml.indent(mapping=2, sequence=4, offset=2)\n",
    "\n",
    "log_info = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_gen = torch.device(\"cuda:0\")\n",
    "device_eval = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup codebook generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    'assert_shapes': True,\n",
    "    'experiment_prefix': 'rl_bac',\n",
    "    \n",
    "    'min_rel_symbol_weight_s': [0.125, 0.125],\n",
    "    'max_rel_symbol_weight_s': [0.875, 0.875],\n",
    "    'min_n_types': 50,  # 16\n",
    "    'max_n_types': 300,  # 256\n",
    "    'min_code_length': 16,  # 6\n",
    "    'max_code_length': 16,  # 16\n",
    "    'min_source_nonuniformity': 10.,  # 1.\n",
    "    'max_source_nonuniformity': 1000.,\n",
    "    \n",
    "    'experiments_per_iter': 1,\n",
    "    'batch_size': 10,  # 4\n",
    "    'n_iters': 1_000_000,\n",
    "    'action_policy': 'sampled',\n",
    "    'action_policy_epsilon': 0.5,\n",
    "    'use_running_average_baseline': False,\n",
    "    'baseline_ma_beta': 0.9,\n",
    "    \n",
    "    # BAC channel\n",
    "    'channel_model': 'channel_bac_merfish',\n",
    "    'decoder_type': 'posterior_sampled',\n",
    "    'decoder_kwargs': {\n",
    "        'split_size': 1},  # 1\n",
    "\n",
    "    # Gaussian channel\n",
    "    # 'channel_model': 'channel_gaussian_merfish',\n",
    "    # 'decoder_type': 'posterior_sampled',\n",
    "    # 'decoder_kwargs': {\n",
    "    #     'n_samples_per_type': 1_000,\n",
    "    #     'max_n_samples_per_type_per_sampling_round': 1_000},\n",
    "\n",
    "    'optim_type': 'adam',\n",
    "    'optim_kwargs': {\n",
    "        'lr': 1e-2,\n",
    "        'betas': (0.5, 0.9)},\n",
    "\n",
    "    # 'optim_type': 'rmsprop',\n",
    "    # 'optim_kwargs': {'lr': 1e-3, 'alpha': 0.99},\n",
    "\n",
    "    # 'optim_type': 'adam',\n",
    "    # 'optim_kwargs': {'lr': 1e-3, 'betas': (0.5, 0.99)},\n",
    "\n",
    "    # 'optim_type': 'rmsprop',\n",
    "    # 'optim_kwargs': {'lr': 1e-4, 'alpha': 0.99},\n",
    "    \n",
    "    'metrics_dict_type': 'basic',\n",
    "    'optimality_type': 'fdr',\n",
    "    'metrics_kwargs': {},\n",
    "\n",
    "    'disable_two_body': True,  # False <======== NOTE\n",
    "    'enable_hardcore_two_body_potential': True,\n",
    "    'two_body_max_n_interactions': 64,\n",
    "    'two_body_dropout_policy': 'random',\n",
    "\n",
    "    'top_proposals_per_column': 512,\n",
    "    'random_proposals_per_column': 512,\n",
    "    'top_k_noise_std': 0.5,\n",
    "    \n",
    "    'baseline_reduction_type': 'mean',\n",
    "    \n",
    "    'log_frequency': 1,\n",
    "    'log_sig_digits': 4,\n",
    "    'log_column_width_large': 30,\n",
    "    'log_column_width_small': 16,\n",
    "    'log_optimality_scale': 100.,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all channel models\n",
    "all_channel_model_specs_dict: Dict[str, ChannelModelSpecification] = {}\n",
    "for k, v in experiments.__dict__.items():\n",
    "    if isinstance(v, ChannelModelSpecification):\n",
    "        all_channel_model_specs_dict[k] = v\n",
    "        \n",
    "# select channel model\n",
    "channel_spec = all_channel_model_specs_dict[params['channel_model']]\n",
    "channel_model = channel_spec.channel_model.to(device_eval).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-body potential\n",
    "one_body_nn_specs = [\n",
    "    (6, 5),\n",
    "    'elu',\n",
    "    (5, 5),\n",
    "    'elu', \n",
    "    (5, channel_model.n_symbols + 1)]\n",
    "\n",
    "one_body_lp_norm = 2\n",
    "\n",
    "one_body_potential = NeuralPIOneBodyMRFPotential(\n",
    "    n_symbols=channel_model.n_symbols,\n",
    "    nn_specs=one_body_nn_specs,\n",
    "    lp_norm=one_body_lp_norm,\n",
    "    assert_shapes=params['assert_shapes']).to(device_gen)\n",
    "\n",
    "# two-body potential\n",
    "n_components = 1\n",
    "\n",
    "two_body_potential_prefactor_provider = BiLinearTwoBodyPotentialPrefactor(\n",
    "    n_components=n_components,\n",
    "    init_bilinear_scale=1.0,\n",
    "    init_linear_scale=0.01,\n",
    "    init_constant_scale=0.01).to(device_gen)\n",
    "\n",
    "two_body_potential = ExpBiLinearPITwoBodyMRFPotential(\n",
    "    n_symbols=channel_model.n_symbols,\n",
    "    n_components=n_components,\n",
    "    code_length_reduction_type='mean',\n",
    "    prefactor_provider=two_body_potential_prefactor_provider,\n",
    "    init_bilinear_diag_scale=0.05,\n",
    "    init_bilinear_rand_scale=0.0,\n",
    "    init_linear_scale=0.0,\n",
    "    init_constant_scale=-5.0).to(device_gen)\n",
    "\n",
    "# eta_nnet_specs = [(n_symbols + 1, 5), 'elu']\n",
    "# xi_nnet_specs = [(5, 10), 'elu']\n",
    "# psi_nnet_specs = [(10, 1)]\n",
    "\n",
    "# two_body_potential = NeuralGeneralPITwoBodyMRFPotential(\n",
    "#     n_symbols=n_symbols,\n",
    "#     n_meta=0,\n",
    "#     eta_nnet_specs=eta_nnet_specs,\n",
    "#     xi_nnet_specs=xi_nnet_specs,\n",
    "#     psi_nnet_specs=psi_nnet_specs).to(device_gen)\n",
    "\n",
    "\n",
    "# two_body_potential_prefactor_provider = NeuralTwoBodyPotentialPrefactor(\n",
    "#     n_components=n_components,\n",
    "#     eta_nnet_specs=[(1, 5), 'elu'],\n",
    "#     psi_nnet_specs=[(5, n_components)]).to(device_gen)\n",
    "\n",
    "\n",
    "# two_body_potential_prefactor_provider = BiLinearTwoBodyPotentialPrefactorSimple(\n",
    "#     n_components=n_components).to(device_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code space filter\n",
    "code_space_filter_func = lambda c_als, code_length: \\\n",
    "    code_space_filter_func_rel_symbol_weight(\n",
    "        c_als=c_als,\n",
    "        code_length=code_length,\n",
    "        min_rel_symbol_weight_s=np.asarray(params['min_rel_symbol_weight_s']),\n",
    "        max_rel_symbol_weight_s=np.asarray(params['max_rel_symbol_weight_s']))\n",
    "\n",
    "codebook_generator = MRFCodebookGenerator(\n",
    "    one_body_potential=one_body_potential,\n",
    "    two_body_potential=two_body_potential,\n",
    "    n_symbols=channel_model.n_symbols,\n",
    "    code_space_filter_func=code_space_filter_func,\n",
    "    device=device_gen,\n",
    "    dtype=dtype,\n",
    "    assert_shapes=params['assert_shapes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimizer\n",
    "# optim_str_to_obj_map = {\n",
    "#     'adam': torch.optim.Adam,\n",
    "#     'rmsprop': torch.optim.RMSprop\n",
    "# }\n",
    "\n",
    "# optim = optim_str_to_obj_map[params['optim_type']](\n",
    "#     params=list(codebook_generator.parameters()),\n",
    "#     **params['optim_kwargs'])\n",
    "\n",
    "optim = torch.optim.Adam([\n",
    "    {'params': codebook_generator.one_body_potential.parameters(),\n",
    "     'lr': 1e-2,\n",
    "     'betas': (0.5, 0.9)},\n",
    "    {'params': codebook_generator.two_body_potential.parameters(),\n",
    "     'lr': 1e-3,\n",
    "     'betas': (0.5, 0.9)}],\n",
    "    lr=1e-2,\n",
    "    betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codebook_generator.load_state_dict(torch.load('./codebook_generator_state__multi__4.pt'))\n",
    "# optim.load_state_dict(torch.load('./optim__multi__4.pt'))\n",
    "\n",
    "# # override two-body parameters\n",
    "# two_body_potential.gamma_m.data = torch.tensor([-1.]).to(device_gen)\n",
    "# two_body_potential.gamma_ms.data = torch.tensor([[0., 0.]]).to(device_gen)\n",
    "# two_body_potential.gamma_mss_unconstrained.data = torch.tensor([[[0.2, 0.], [0., 0.2]]]).to(device_gen)\n",
    "\n",
    "# codebook_generator.load_state_dict(\n",
    "#     torch.load('./codebook_generator_state__greedy__140__MERFISH__full_interaction__test.pt'),\n",
    "#     strict=False)\n",
    "\n",
    "# codebook_generator.load_state_dict(torch.load('./codebook_generator_state__1.pt'))\n",
    "# # optim.load_state_dict(torch.load('./optim__1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_iter = 0\n",
    "optimality_hist = []\n",
    "baseline_hist = []\n",
    "baseline_running_average = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in optim.param_groups:\n",
    "#     g['lr'] = 1e-3\n",
    "#     g['betas'] = (0.5, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params['enable_hardcore_two_body_potential'] = False\n",
    "# params['disable_two_body'] = False\n",
    "# params['baseline_reduction_type'] = 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging parameters\n",
    "log_frequency = params['log_frequency']\n",
    "small_col = params['log_column_width_small']\n",
    "large_col = params['log_column_width_large']\n",
    "sig_digits = params['log_sig_digits']\n",
    "log_optimality_scale = params['log_optimality_scale']\n",
    "\n",
    "header_string = \\\n",
    "    f\"{'i_iter'.ljust(small_col)}\" \\\n",
    "    f\"{'code_length'.ljust(small_col)}\" \\\n",
    "    f\"{'n_types'.ljust(small_col)}\" \\\n",
    "    f\"{'nonuniformity'.ljust(small_col)}\" \\\n",
    "    f\"{'optimality'.ljust(large_col)}\" \\\n",
    "    f\"{'optimality_compl'.ljust(large_col)}\" \\\n",
    "    f\"{'baseline'.ljust(small_col)}\"\n",
    "\n",
    "log_info(f\"Logging optimality scale factor: {log_optimality_scale:.3f}\")\n",
    "log_info(\"\")\n",
    "log_info(header_string)\n",
    "log_info('=' * len(header_string))\n",
    "\n",
    "while i_iter < params['n_iters']:\n",
    "\n",
    "    # zero grad\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # accumulate grad\n",
    "    for i_expmt in range(params['experiments_per_iter']):\n",
    "        \n",
    "        # generate experiment\n",
    "        experiment_spec = experiments.generate_experiment_spec(\n",
    "            name_prefix=params['experiment_prefix'],\n",
    "            min_rel_symbol_weight_s=params['min_rel_symbol_weight_s'],\n",
    "            max_rel_symbol_weight_s=params['max_rel_symbol_weight_s'],\n",
    "            n_symbols=channel_model.n_symbols,\n",
    "            min_code_length=params['min_code_length'],\n",
    "            max_code_length=params['max_code_length'],\n",
    "            min_n_types=params['min_n_types'],\n",
    "            max_n_types=params['max_n_types'],\n",
    "            min_source_nonuniformity=params['min_source_nonuniformity'],\n",
    "            max_source_nonuniformity=params['max_source_nonuniformity'])\n",
    "\n",
    "        # generate problem spec\n",
    "        problem_spec = experiments.SingleEntityCodingProblemSpecification(\n",
    "            experiment_spec=experiment_spec,\n",
    "            channel_spec=channel_spec)\n",
    "\n",
    "        # sample a batch of codebooks\n",
    "        codebook_generator_output_dict = codebook_generator.forward(\n",
    "            code_length=experiment_spec.code_length,\n",
    "            n_types=experiment_spec.n_types,\n",
    "            batch_size=params['batch_size'],\n",
    "            pi_t=experiment_spec.pi_t,\n",
    "            nu_tj=None,\n",
    "            disable_two_body=params['disable_two_body'],\n",
    "            two_body_max_n_interactions=params['two_body_max_n_interactions'],\n",
    "            two_body_dropout_policy=params['two_body_dropout_policy'],\n",
    "            top_proposals_per_column=params['top_proposals_per_column'],\n",
    "            random_proposals_per_column=params['random_proposals_per_column'],\n",
    "            enable_hardcore_two_body_potential=params['enable_hardcore_two_body_potential'],\n",
    "            action_policy=params['action_policy'],\n",
    "            action_policy_epsilon=params['action_policy_epsilon'],\n",
    "            top_k_noise_std=params['top_k_noise_std'])\n",
    "\n",
    "        # get the decoding confusion matrix\n",
    "        decoder_output_dict = channel_model.get_weighted_confusion_matrix(\n",
    "            codebook_btls=codebook_generator_output_dict['codebook_btls'].to(device_eval),\n",
    "            pi_bt=to_torch(experiment_spec.pi_t, device=device_eval, dtype=dtype).expand(\n",
    "                [params['batch_size'], experiment_spec.n_types]),\n",
    "            decoder_type=params['decoder_type'],\n",
    "            **params['decoder_kwargs'])\n",
    "\n",
    "        # get metrics dict\n",
    "        metrics_dict = metric_utils.get_metrics_dict_from_decoder_output_dict(\n",
    "            decoder_output_dict=decoder_output_dict,\n",
    "            metrics_dict_type=params['metrics_dict_type'],\n",
    "            **params['metrics_kwargs'])\n",
    "\n",
    "        # reduce metrics dict to optimality\n",
    "        optimality_b = metric_utils.get_optimality_from_metrics_dict(\n",
    "            metrics_dict=metrics_dict,\n",
    "            optimality_type=params['optimality_type'])\n",
    "\n",
    "        # baseline\n",
    "        baseline_reduction = {\n",
    "            'mean': torch.mean,\n",
    "            'median': torch.median}[params['baseline_reduction_type']]\n",
    "        optimality_batch_baseline = baseline_reduction(optimality_b).item()\n",
    "        baseline_running_average = (\n",
    "            params['baseline_ma_beta'] * baseline_running_average + \n",
    "            (1. - params['baseline_ma_beta']) * optimality_batch_baseline)\n",
    "        baseline_running_average_unbiased = baseline_running_average / (1. - params['baseline_ma_beta'] ** (i_iter + 1))\n",
    "        baseline = baseline_running_average_unbiased if params['use_running_average_baseline'] else optimality_batch_baseline\n",
    "        optimality_sub_baseline_b = optimality_b - baseline\n",
    "\n",
    "        # REINFORCE\n",
    "        loss = -torch.dot(\n",
    "            codebook_generator_output_dict['log_prob_b'],\n",
    "            optimality_sub_baseline_b.to(device_gen))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # bookkeeping\n",
    "        optimality_hist.append(optimality_batch_baseline)\n",
    "        baseline_hist.append(baseline_running_average_unbiased)\n",
    "\n",
    "        if i_iter % params['log_frequency'] == 0:\n",
    "\n",
    "            source_nonuniformity = (experiment_spec.pi_t[0] / experiment_spec.pi_t[-1]).item()\n",
    "            log_optimality_batch_mean = log_optimality_scale * optimality_b.mean().item()\n",
    "            log_optimality_batch_lo = log_optimality_scale * optimality_b.min().item() \n",
    "            log_optimality_batch_hi = log_optimality_scale * optimality_b.max().item() \n",
    "            log_optimality_compl_batch_mean = log_optimality_scale * (1. - optimality_b).mean().item() \n",
    "            log_optimality_compl_batch_lo = log_optimality_scale * (1. - optimality_b).min().item() \n",
    "            log_optimality_compl_batch_hi = log_optimality_scale * (1. - optimality_b).max().item()\n",
    "            log_baseline = log_optimality_scale * baseline_running_average_unbiased\n",
    "\n",
    "            log_string = \\\n",
    "                f\"{str(i_iter).ljust(small_col)}\" + \\\n",
    "                f\"{experiment_spec.code_length}\".ljust(small_col) + \\\n",
    "                f\"{experiment_spec.n_types}\".ljust(small_col) + \\\n",
    "                f\"{source_nonuniformity:.{sig_digits}f}\".ljust(small_col) + \\\n",
    "                f\"{log_optimality_batch_mean:.{sig_digits}f} ({log_optimality_batch_lo:.{sig_digits}f}, {log_optimality_batch_hi:.{sig_digits}f})\".ljust(large_col) + \\\n",
    "                f\"{log_optimality_compl_batch_mean:.{sig_digits}f} ({log_optimality_compl_batch_lo:.{sig_digits}f}, {log_optimality_compl_batch_hi:.{sig_digits}f})\".ljust(large_col) + \\\n",
    "                f\"{log_baseline:.{sig_digits}f}\".ljust(small_col);\n",
    "\n",
    "            log_info(log_string)\n",
    "\n",
    "    # gradient update\n",
    "    optim.step()\n",
    "        \n",
    "    i_iter += 1\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(optimality_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(baseline_hist)\n",
    "plt.ylim((0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_body_potential_prefactor_provider.beta_constant_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_body_potential_prefactor_provider.beta_linear_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_body_potential_prefactor_provider.beta_bilinear_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_body_potential.gamma_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_body_potential.gamma_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_body_potential.gamma_mss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(codebook_generator.state_dict(), './codebook_generator_state__multi__16.pt')\n",
    "torch.save(optim.state_dict(), './optim__multi__16.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codebook_generator.load_state_dict(\n",
    "#     torch.load('./codebook_generator_state__greedy__140__MERFISH__full_interaction__test.pt'),\n",
    "#     strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # generate experiment\n",
    "    experiment_spec = experiments.generate_experiment_spec(\n",
    "        name_prefix=params['experiment_prefix'],\n",
    "        min_rel_symbol_weight_s=params['min_rel_symbol_weight_s'],\n",
    "        max_rel_symbol_weight_s=params['max_rel_symbol_weight_s'],\n",
    "        n_symbols=channel_model.n_symbols,\n",
    "        min_code_length=16,\n",
    "        max_code_length=16,\n",
    "        min_n_types=140,\n",
    "        max_n_types=140,\n",
    "        min_source_nonuniformity=1000.,\n",
    "        max_source_nonuniformity=1000.)\n",
    "\n",
    "    # generate problem spec\n",
    "    problem_spec = experiments.SingleEntityCodingProblemSpecification(\n",
    "        experiment_spec=experiment_spec,\n",
    "        channel_spec=channel_spec)\n",
    "\n",
    "    # sample a batch of codebooks\n",
    "    codebook_generator_output_dict = codebook_generator.forward(\n",
    "        code_length=experiment_spec.code_length,\n",
    "        n_types=experiment_spec.n_types,\n",
    "        batch_size=1,\n",
    "        pi_t=experiment_spec.pi_t,\n",
    "        nu_tj=None,\n",
    "        disable_two_body=params['disable_two_body'],   # NOTE\n",
    "        two_body_max_n_interactions=256,\n",
    "        two_body_dropout_policy='random',\n",
    "        top_proposals_per_column=2048,\n",
    "        random_proposals_per_column=2048,\n",
    "        enable_hardcore_two_body_potential=True,\n",
    "        action_policy='greedy',\n",
    "        action_policy_epsilon=0.,\n",
    "        top_k_noise_std=0.)\n",
    "\n",
    "    # get the decoding confusion matrix\n",
    "    decoder_output_dict = channel_model.get_weighted_confusion_matrix(\n",
    "        codebook_btls=codebook_generator_output_dict['codebook_btls'].to(device_eval),\n",
    "        pi_bt=to_torch(experiment_spec.pi_t, device=device_eval, dtype=dtype).expand(\n",
    "            [1, experiment_spec.n_types]),\n",
    "        decoder_type=params['decoder_type'],\n",
    "        **params['decoder_kwargs'])\n",
    "\n",
    "    # get metrics dict\n",
    "    metrics_dict = metric_utils.get_metrics_dict_from_decoder_output_dict(\n",
    "        decoder_output_dict=decoder_output_dict,\n",
    "        metrics_dict_type=params['metrics_dict_type'],\n",
    "        **params['metrics_kwargs'])\n",
    "\n",
    "    # reduce metrics dict to optimality\n",
    "    optimality_b = metric_utils.get_optimality_from_metrics_dict(\n",
    "        metrics_dict=metrics_dict,\n",
    "        optimality_type=params['optimality_type'])\n",
    "\n",
    "# baseline\n",
    "best_idx = torch.argmax(optimality_b)\n",
    "print(f'FDR: {1. - optimality_b[best_idx]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_btls = codebook_generator_output_dict['codebook_btls']\n",
    "idx = 0\n",
    "\n",
    "# get binary codebook\n",
    "codebook_tl = codebook_btls[idx, :, :, 1].detach().cpu().numpy()\n",
    "\n",
    "# sort codebook\n",
    "codebook_tl = codebook_tl[:, np.lexsort(codebook_tl[::-1, :])[::-1]]\n",
    "\n",
    "# aux\n",
    "hamming_weight_t = codebook_tl.sum(-1)\n",
    "\n",
    "# one-body potential\n",
    "with torch.no_grad():\n",
    "    one_body_props = one_body_potential.get_one_body_potential_props(\n",
    "        n_types_b=torch.tensor(\n",
    "            experiment_spec.n_types, device=device_gen, dtype=dtype).expand([experiment_spec.n_types]),\n",
    "        code_length_b=torch.tensor(\n",
    "            experiment_spec.code_length, device=device_gen, dtype=dtype).expand([experiment_spec.n_types]),\n",
    "        type_rank_b=torch.arange(\n",
    "            experiment_spec.n_types, device=device_gen, dtype=dtype) / experiment_spec.n_types,\n",
    "        pi_b=torch.tensor(\n",
    "            experiment_spec.pi_t, device=device_gen, dtype=dtype),\n",
    "        pi_cdf_b=torch.cumsum(torch.tensor(\n",
    "            experiment_spec.pi_t, device=device_gen, dtype=dtype), -1))\n",
    "\n",
    "# make figure\n",
    "fig, axs = plt.subplots(nrows=3, figsize=(14, 7))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.imshow(codebook_tl.T,  cmap=plt.cm.gray)\n",
    "ax.set_xlabel('Rank (descending source prior)')\n",
    "ax.set_ylabel('Code')\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim((-0.5, experiment_spec.n_types - 0.5))\n",
    "    \n",
    "ax = axs[1]\n",
    "ax.plot(hamming_weight_t)\n",
    "ax.plot(experiment_spec.code_length * one_body_props['symbol_weights_bs'][:, 1].cpu().numpy())\n",
    "ax.set_xlabel('Rank (descending source prior)')\n",
    "ax.set_ylabel('Hamming Weight')\n",
    "ax.set_ylim((0, experiment_spec.code_length))\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim((-0.5, experiment_spec.n_types - 0.5))\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(one_body_props['potential_strength_b'].cpu().numpy(), color='red')\n",
    "ax2.set_xlabel('Rank (descending source prior)')\n",
    "ax2.set_ylabel('Strength', color='red')\n",
    "ax2.set_xlim((-0.5, experiment_spec.n_types - 0.5))\n",
    "\n",
    "ax = axs[2]\n",
    "ax.bar(np.arange(experiment_spec.n_types), metrics_dict['fdr_bt'][idx, :].cpu().numpy())\n",
    "ax.set_xlabel('Rank (descending source prior)')\n",
    "ax.set_ylabel('FDR')\n",
    "ax.set_xlim((-0.5, experiment_spec.n_types - 0.5))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# load SA data\n",
    "root_path = '/home/jupyter/mb-ml-dev-disk/MightyCodes/notebooks/terra_analysis'\n",
    "sa_bac_df = pd.read_csv(os.path.join(root_path, 'sa_bac_df.tsv'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_energy_var = []\n",
    "\n",
    "for i, row in sa_bac_df.iterrows():\n",
    "    print(f'{i} ...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # generate experiment\n",
    "        experiment_spec = experiments.generate_experiment_spec(\n",
    "            name_prefix='test',\n",
    "            min_symbol_weight_s=[row['min_hamming_weight'], row['min_hamming_weight']],\n",
    "            max_symbol_weight_s=[row['max_hamming_weight'], row['max_hamming_weight']],\n",
    "            n_symbols=2,\n",
    "            min_code_length=row['code_length'],\n",
    "            max_code_length=row['code_length'],\n",
    "            min_n_types=row['n_types'],\n",
    "            max_n_types=row['n_types'],\n",
    "            min_source_nonuniformity=row['source_nonuniformity'],\n",
    "            max_source_nonuniformity=row['source_nonuniformity'])\n",
    "\n",
    "        # generate problem spec\n",
    "        problem_spec = experiments.SingleEntityCodingProblemSpecification(\n",
    "            experiment_spec=experiment_spec,\n",
    "            channel_spec=channel_spec)\n",
    "\n",
    "        # sample a batch of codebooks\n",
    "        codebook_generator_output_dict = codebook_generator.forward(\n",
    "            code_length=experiment_spec.code_length,\n",
    "            n_types=experiment_spec.n_types,\n",
    "            batch_size=1,\n",
    "            pi_t=experiment_spec.pi_t,\n",
    "            nu_tj=None,\n",
    "            disable_two_body=False,\n",
    "            two_body_max_n_interactions=row['n_types'],\n",
    "            two_body_dropout_policy='random',\n",
    "            top_proposals_per_column=1024,\n",
    "            random_proposals_per_column=1024,\n",
    "            enable_hardcore_two_body_potential=True,\n",
    "            action_policy='greedy',\n",
    "            action_policy_epsilon=0.,\n",
    "            top_k_noise_std=0.)\n",
    "\n",
    "        # get the decoding confusion matrix\n",
    "        decoder_output_dict = channel_model.get_weighted_confusion_matrix(\n",
    "            codebook_btls=codebook_generator_output_dict['codebook_btls'].to(device_eval),\n",
    "            pi_bt=to_torch(experiment_spec.pi_t, device=device_eval, dtype=dtype).expand(\n",
    "                [1, experiment_spec.n_types]),\n",
    "            decoder_type=params['decoder_type'],\n",
    "            **params['decoder_kwargs'])\n",
    "\n",
    "        # get metrics dict\n",
    "        metrics_dict = metric_utils.get_metrics_dict_from_decoder_output_dict(\n",
    "            decoder_output_dict=decoder_output_dict,\n",
    "            metrics_dict_type=params['metrics_dict_type'],\n",
    "            **params['metrics_kwargs'])\n",
    "\n",
    "        # reduce metrics dict to optimality\n",
    "        optimality_b = metric_utils.get_optimality_from_metrics_dict(\n",
    "            metrics_dict=metrics_dict,\n",
    "            optimality_type=params['optimality_type'])\n",
    "        \n",
    "        # add to list\n",
    "        optimality_compl = 1. - optimality_b.item()\n",
    "        lowest_energy_var.append(optimality_compl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_bac_df['lowest_energy_var'] = np.asarray(lowest_energy_var)\n",
    "sa_bac_df.to_csv(os.path.join(root_path, 'sa_bac_w_var_df.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = estimate_codebook_auc_f1_reject(\n",
    "    model=model,\n",
    "    c_btls=output_dict['c_btls'].to(device_eval),\n",
    "    pi_bt=pi_t.expand((1, n_types)).to(device_eval),\n",
    "    n_samples_per_type=100_000,\n",
    "    n_map_reject_thresholds=50,\n",
    "    delta_q_max=1e-4,\n",
    "    max_rej_ratio=max_rej_ratio,\n",
    "    max_n_samples_per_type_per_sampling_round=100,\n",
    "    return_confusion_matrix=True,\n",
    "    device=device_eval,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_metrics_dict(metrics_dict, 0, x_key='clamped_rej', y_key='clamped_f_1', x_label='Rejection Rate', y_label='$F_1$ Score')\n",
    "plt.xlim((0, 0.25))\n",
    "plt.ylim((0.85, 1.01))\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/pi_nu_300_700__opt_f1_score.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1, fig_2, fig_3 = viz_binary_codebook_and_metrics(metrics_dict, 0, output_dict['c_btls'], metric_key='fdr', figsize=(12, 3))\n",
    "\n",
    "fig_1.gca().set_ylabel('Code')\n",
    "fig_1.gca().set_xlabel('Rank')\n",
    "fig_1.gca().set_yticks([])\n",
    "fig_1.tight_layout()\n",
    "fig_1.savefig('./figs/pi_nu_300_700__opt_code.png', dpi=300)\n",
    "\n",
    "fig_3.gca().set_ylabel('FDR')\n",
    "fig_3.gca().set_ylim((0, 0.1))\n",
    "fig_3.tight_layout()\n",
    "fig_3.savefig('./figs/pi_nu_300_700__opt_fdr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict['normalized_auc_f_1_rej_bt'].mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tl = output_dict['c_btls'][0, :, :, 1]\n",
    "z_tl = torch.cumsum(c_tl, dim=0)\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.imshow(to_np(z_tl).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHD4 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MHD4 codebook\n",
    "MERFISH_coding_data_root = '/home/jupyter/mb-ml-data/MERFISH-coding'\n",
    "mhd4_codebook_krc = np.load(\n",
    "    os.path.join(MERFISH_coding_data_root, 'MERFISH_2016_U2OS_1_codebook_krc.npy'))\n",
    "n_codes_mhd4 = mhd4_codebook_krc.shape[0]\n",
    "mhd4_codebook_tl = mhd4_codebook_krc.reshape(n_codes_mhd4, -1)\n",
    "\n",
    "# one-hot representation\n",
    "mhd4_codebook_tls = np.eye(2)[mhd4_codebook_tl.flatten(), :].reshape(mhd4_codebook_tl.shape + (2,))\n",
    "mhd4_c_btls = to_torch(\n",
    "    mhd4_codebook_tls[np.random.permutation(n_codes_mhd4), :, :][None, ...],\n",
    "    device=device_eval, dtype=dtype)\n",
    "\n",
    "mhd4_metrics_dict = estimate_codebook_auc_f1_reject(\n",
    "    model=model,\n",
    "    c_btls=mhd4_c_btls,\n",
    "    pi_bt=pi_t.expand((1, n_types)),\n",
    "    n_samples_per_type=50_000,\n",
    "    n_map_reject_thresholds=50,\n",
    "    delta_q_max=delta_q_max,\n",
    "    max_rej_ratio=max_rej_ratio,\n",
    "    max_n_samples_per_type_per_sampling_round=100,\n",
    "    return_confusion_matrix=False,\n",
    "    device=device_eval,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_metrics_dict(mhd4_metrics_dict, 0, x_key='clamped_rej', y_key='clamped_f_1', x_label='Rejection Rate', y_label='$F_1$ Score')\n",
    "plt.xlim((0, 0.25))\n",
    "plt.ylim((0.85, 1.01))\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/pi_nu_300__mhd4_f1_score.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhd4_metrics_dict['normalized_auc_f_1_rej_bt'].mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1, fig_2, fig_3 = viz_binary_codebook_and_metrics(mhd4_metrics_dict, 0, mhd4_c_btls, metric_key='fdr', figsize=(12, 3))\n",
    "\n",
    "fig_1.gca().set_ylabel('Code')\n",
    "fig_1.gca().set_xlabel('Rank')\n",
    "fig_1.gca().set_yticks([])\n",
    "fig_1.tight_layout()\n",
    "fig_1.savefig('./figs/pi_nu_300__mhd4_code.png')\n",
    "\n",
    "fig_3.gca().set_ylabel('FDR')\n",
    "fig_3.gca().set_ylim((0, 0.1))\n",
    "fig_3.tight_layout()\n",
    "fig_3.savefig('./figs/pi_nu_300__mhd4_fdr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tl = mhd4_c_btls[0, :, :, 1]\n",
    "z_tl = torch.cumsum(c_tl, dim=0)\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.imshow(to_np(z_tl).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "barlist = plt.bar(np.arange(n_types) + 1, to_np(pi_t), width=0.5)\n",
    "red = np.asarray([1., 0., 0.])\n",
    "blue = np.asarray([0., 0., 1.])\n",
    "for i_t, bar in enumerate(barlist):\n",
    "    color = red * (1 - i_t / (n_types - 1)) + blue * i_t / (n_types - 1)\n",
    "    bar.set_color(color)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Source Prior')\n",
    "plt.xlim((1, n_types + 1))\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/pi_nu_300_700_prior.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = metrics_dict['weighted_confusion_matrix_bqtu'][0, 10, :, :]\n",
    "w = w / w.sum(-1, keepdim=True)\n",
    "plt.imshow(to_np(w), vmax=0.5, cmap=plt.cm.gray)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
