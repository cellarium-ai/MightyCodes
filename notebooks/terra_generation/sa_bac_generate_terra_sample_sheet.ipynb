{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sample table for Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "from typing import Any\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values\n",
    "sample_set_id = 'second_wave'\n",
    "experiment_prefix = 'sa_bac'\n",
    "base_yaml_file = 'gs://fc-secure-edbbad6f-85d8-45a4-8e64-3f4a0ac501ff/base_yaml_files/sa_bac_template_params_second_wave.yaml'\n",
    "mighty_codes_tar_gz = 'gs://fc-secure-edbbad6f-85d8-45a4-8e64-3f4a0ac501ff/mighty_codes_tarballs/MightyCodes-e8303bd.tar.gz'\n",
    "final_state = 'N/A'\n",
    "checkpoint_interval_seconds = 1000\n",
    "eval_split_size = 128\n",
    "convergence_abs_tol = 1e-6\n",
    "convergence_countdown = 10\n",
    "\n",
    "# where to save the .tsv file\n",
    "sample_output_path = f'../ws/sa_bac_{sample_set_id}_{experiment_prefix}_sample.tsv'\n",
    "sample_set_entity_output_path = f'../ws/sa_bac_{sample_set_id}_{experiment_prefix}_sample_set_entity.tsv'\n",
    "sample_set_membership_output_path = f'../ws/sa_bac_{sample_set_id}_{experiment_prefix}_sample_set_membership.tsv'\n",
    "\n",
    "# batch settings\n",
    "code_length_list = [\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10\n",
    "]\n",
    "\n",
    "min_max_hamming_weight_dict = {\n",
    "    6: [[1, 5], [2, 4]],\n",
    "    7: [[1, 6], [2, 5]],\n",
    "    8: [[1, 7], [2, 6]],\n",
    "    9: [[1, 8], [2, 7]],\n",
    "    10: [[1, 9], [2, 8]]\n",
    "}\n",
    "\n",
    "n_types_dict = {\n",
    "    6: [16],\n",
    "    7: [32, 16],\n",
    "    8: [64, 32, 16],\n",
    "    9: [128, 64, 32, 16],\n",
    "    10: [256, 128, 64, 32, 16]\n",
    "}\n",
    "\n",
    "source_nonuniformity_list = [\n",
    "    10.,\n",
    "    100.,\n",
    "    1000.\n",
    "]\n",
    "\n",
    "channel_model_list = [\n",
    "    'channel_bac_merfish',\n",
    "    # 'channel_bsc_10'\n",
    "]\n",
    "\n",
    "quality_factor_list = [\n",
    "    10\n",
    "]\n",
    "\n",
    "metric_type_list = [\n",
    "    'fdr',\n",
    "    # 'tpr',\n",
    "    # 'fdr[0.25]'\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    'entity:sample_id',\n",
    "    'base_yaml_file',\n",
    "    'mighty_codes_tar_gz',\n",
    "    'experiment_prefix',\n",
    "    'channel_model',\n",
    "    'quality_factor',\n",
    "    'convergence_abs_tol',\n",
    "    'convergence_countdown',\n",
    "    'code_length',\n",
    "    'min_hamming_weight',\n",
    "    'max_hamming_weight',\n",
    "    'n_types',\n",
    "    'source_nonuniformity',\n",
    "    'metric_type',\n",
    "    'checkpoint_interval_seconds',\n",
    "    'eval_split_size',\n",
    "    'final_state'\n",
    "]\n",
    "\n",
    "identity = lambda x: x\n",
    "\n",
    "primitive_opts = [\n",
    "    ('base_yaml_file', [base_yaml_file]),\n",
    "    ('mighty_codes_tar_gz', [mighty_codes_tar_gz]),\n",
    "    ('experiment_prefix', [experiment_prefix]),\n",
    "    ('channel_model', channel_model_list),\n",
    "    ('quality_factor', quality_factor_list),\n",
    "    ('convergence_abs_tol', [convergence_abs_tol]),\n",
    "    ('convergence_countdown', [convergence_countdown]),\n",
    "    ('code_length', code_length_list),\n",
    "    ('source_nonuniformity', source_nonuniformity_list),\n",
    "    ('metric_type', metric_type_list),\n",
    "    ('checkpoint_interval_seconds', [checkpoint_interval_seconds]),\n",
    "    ('eval_split_size', [eval_split_size]),\n",
    "    ('final_state', [final_state])\n",
    "]\n",
    "\n",
    "conditional_opts = [\n",
    "    ('code_length', min_max_hamming_weight_dict,\n",
    "     [('min_hamming_weight', operator.itemgetter(0)),\n",
    "      ('max_hamming_weight', operator.itemgetter(1))]),\n",
    "    ('code_length', n_types_dict,\n",
    "     [('n_types', identity)])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation list\n",
    "evals = []\n",
    "\n",
    "# process primitive options\n",
    "for opt in primitive_opts:\n",
    "    assert isinstance(opt, tuple)\n",
    "    assert len(opt) == 2\n",
    "    assert isinstance(opt[0], str)\n",
    "    assert isinstance(opt[1], list)\n",
    "    key = opt[0]\n",
    "    values = opt[1]\n",
    "    next_evals = []\n",
    "    for value in values:\n",
    "        if len(evals) > 0:\n",
    "            for prev in evals:\n",
    "                new = deepcopy(prev)\n",
    "                new[key] = value\n",
    "                next_evals.append(new)\n",
    "        else:\n",
    "            new = {key: value}\n",
    "            next_evals.append(new)\n",
    "    evals = next_evals\n",
    "\n",
    "# process conditional options\n",
    "assert len(evals) > 0\n",
    "for opt in conditional_opts:\n",
    "    assert isinstance(opt, tuple)\n",
    "    assert len(opt) == 3\n",
    "    parent_key = opt[0]\n",
    "    parent_to_children_value_dict = opt[1]\n",
    "    children_valuation_manifest_list = opt[2]\n",
    "    assert isinstance(parent_key, str)\n",
    "    assert isinstance(parent_to_children_value_dict, dict)\n",
    "    assert isinstance(children_valuation_manifest_list, list)\n",
    "    next_evals = []\n",
    "    for prev in evals:\n",
    "        assert parent_key in prev\n",
    "        parent_value = prev[parent_key]\n",
    "        assert parent_value in parent_to_children_value_dict\n",
    "        children_value_bundle_list = parent_to_children_value_dict[parent_value]\n",
    "        assert isinstance(children_value_bundle_list, list)\n",
    "        for children_value_bundle in children_value_bundle_list:\n",
    "            new = deepcopy(prev)\n",
    "            for child_key, child_value_extractor in children_valuation_manifest_list:\n",
    "                assert child_key not in new, f\"{child_key}, {new}\"\n",
    "                new[child_key] = child_value_extractor(children_value_bundle)\n",
    "            next_evals.append(new)\n",
    "    evals = next_evals\n",
    "    \n",
    "# generate names\n",
    "for e in evals:\n",
    "    name = (f\"{experiment_prefix}__\"\n",
    "            f\"{sample_set_id}__\"\n",
    "            f\"{e['channel_model']}__\"\n",
    "            f\"{e['code_length']}__{e['min_hamming_weight']}__{e['max_hamming_weight']}__\"\n",
    "            f\"{e['n_types']}__\"\n",
    "            f\"{int(e['source_nonuniformity'])}__\"\n",
    "            f\"{e['quality_factor']}__\"\n",
    "            f\"{e['metric_type']}\")\n",
    "    e['entity:sample_id'] = name\n",
    "    e['entity:sample_set_id'] = sample_set_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate .tsv file\n",
    "with open(sample_output_path, 'w') as f:\n",
    "    f.write('\\t'.join(columns) + '\\n')\n",
    "    for e in evals:\n",
    "        values = [str(e[column]) for column in columns]\n",
    "        f.write('\\t'.join(values) + '\\n')\n",
    "        \n",
    "with open(sample_set_entity_output_path, 'w') as f:\n",
    "    f.write('entity:sample_set_id\\n')\n",
    "    f.write(f'{sample_set_id}\\n')\n",
    "    for code_length in code_length_list:\n",
    "        f.write(f'{sample_set_id}_{code_length}\\n')\n",
    "    \n",
    "with open(sample_set_membership_output_path, 'w') as f:\n",
    "    f.write('membership:sample_set_id\\tsample\\n')\n",
    "    for e in evals:\n",
    "        f.write(f\"{sample_set_id}\\t{e['entity:sample_id']}\\n\")\n",
    "    for code_length in code_length_list:\n",
    "        for e in evals:\n",
    "            if int(e['code_length']) == code_length:\n",
    "                f.write(f\"{sample_set_id}_{code_length}\\t{e['entity:sample_id']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
